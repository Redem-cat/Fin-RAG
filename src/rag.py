# RAG architecture using LangChain, Ollama and Elasticsearch
# Modified by Redem-cat

import os
from datetime import datetime, timedelta
from pathlib import Path
import json

import numpy as np
from dotenv import load_dotenv

from langchain_elasticsearch import ElasticsearchStore
from langchain_ollama import OllamaEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict

# =========================
# ğŸ”¹ æ£€ç´¢æ—¥å¿—ç®¡ç†å™¨
# =========================
class RetrievalLogger:
    """æ£€ç´¢æ—¥å¿—ç®¡ç†å™¨ï¼šè®°å½•æ£€ç´¢è¯¦æƒ…å¹¶å®šæœŸæ¸…ç†"""

    def __init__(self, log_dir: str = None, max_log_files: int = 10):
        if log_dir is None:
            log_dir = base_path / "retrieval_logs"
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.max_log_files = max_log_files
        self.session_count = 0

    def log(self, question: str, retrieved_docs: list, answer: str, used_context: bool):
        """è®°å½•ä¸€æ¬¡æ£€ç´¢çš„è¯¦ç»†ä¿¡æ¯"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.log_dir / f"retrieval_{timestamp}.json"

        # å‡†å¤‡æ—¥å¿—æ•°æ®
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "retrieved_docs": [],
            "answer": answer,
            "used_context": used_context
        }

        # å¤„ç†æ£€ç´¢åˆ°çš„æ–‡æ¡£
        for doc in retrieved_docs:
            if isinstance(doc, tuple):
                document, score = doc
                log_data["retrieved_docs"].append({
                    "content": document.page_content[:500],  # åªä¿å­˜å‰500å­—ç¬¦
                    "metadata": document.metadata,
                    "raw_score": score
                })
            else:
                log_data["retrieved_docs"].append({
                    "content": doc.page_content[:500],
                    "metadata": doc.metadata,
                    "raw_score": None
                })

        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)

        # æ¯æ¬¡è®°å½•åæ¸…ç†ï¼Œä¿æŒæœ€å¤š max_log_files ä¸ª
        self.clean_old_logs()

    def clean_old_logs(self):
        """æ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶ï¼Œä¿ç•™æœ€è¿‘çš„ max_log_files ä¸ª"""
        log_files = list(self.log_dir.glob("retrieval_*.json"))
        log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

        if len(log_files) > self.max_log_files:
            for old_file in log_files[self.max_log_files:]:
                old_file.unlink()




# =========================
# ğŸ”¹ é…ç½®å’Œåˆå§‹åŒ–
# =========================
base_path = Path(__file__).parent.parent.resolve()
retrieval_logger = RetrievalLogger(max_log_files=10)

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = Path(base_path / "elastic-start-local/.env")
if not dotenv_path.is_file():
    print("Error: it seems Elasticsearch has not been installed")
    print("using start-local, please execute the following command:")
    print("curl -fsSL https://elastic.co/start-local | sh")
    exit(1)
    
load_dotenv(dotenv_path=dotenv_path)
index_name = "rag-langchain"

# Embeddings
embeddings = OllamaEmbeddings(
    model="my-bge-m3",
)

# LLM
llm = ChatOllama(model="my-qwen25", temperature=0.0000000001)


# =========================
# ğŸ”¹ å¯¹è¯å†å²ç®¡ç†å™¨ï¼ˆæ··åˆæ£€ç´¢ + åˆ†å±‚å­˜å‚¨ï¼‰
# =========================
class MemoryManager:
    """å¯¹è¯å†å²ç®¡ç†å™¨ï¼šæ··åˆæ£€ç´¢ + åˆ†å±‚å­˜å‚¨"""
    
    def __init__(self, memory_dir: str = None, compaction_interval: int = 10):
        if memory_dir is None:
            memory_dir = base_path / "memory"
        self.memory_dir = Path(memory_dir)
        self.memory_dir.mkdir(exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„
        self.soul_file = self.memory_dir / "SOUL.md"
        self.agents_file = self.memory_dir / "AGENTS.md"
        self.memory_file = self.memory_dir / "MEMORY.md"
        self.daily_dir = self.memory_dir / "daily"
        self.daily_dir.mkdir(exist_ok=True)
        
        # compaction è®¾ç½®
        self.compaction_interval = compaction_interval
        self.conversation_count = 0
        
        # åˆå§‹åŒ–å¿…è¦æ–‡ä»¶
        self._ensure_files()
    
    def _ensure_files(self):
        """ç¡®ä¿å¿…è¦æ–‡ä»¶å­˜åœ¨"""
        if not self.soul_file.exists():
            self.soul_file.write_text("# AI çµé­‚é…ç½®\n", encoding="utf-8")
        if not self.agents_file.exists():
            self.agents_file.write_text("# Agent è§„èŒƒ\n", encoding="utf-8")
        if not self.memory_file.exists():
            self.memory_file.write_text("# é•¿æœŸè®°å¿†\n\n## ç”¨æˆ·åå¥½\n\n## æ ¸å¿ƒäº‹å®\n\n## å…³é”®å†³ç­–\n\n", encoding="utf-8")
    
    def _get_today_file(self) -> Path:
        """è·å–ä»Šæ—¥æ—¥å¿—æ–‡ä»¶"""
        today = datetime.now().strftime("%Y-%m-%d")
        return self.daily_dir / f"{today}.md"
    
    def _extract_keywords(self, text: str) -> set:
        """ç®€å•å…³é”®è¯æå–ï¼ˆåŸºäºå­—ç¬¦åˆ†å‰²ï¼‰"""
        # ç§»é™¤æ ‡ç‚¹ï¼Œåˆ†å‰²æˆè¯
        import re
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text)
        # è¿‡æ»¤çŸ­è¯
        keywords = {w.lower() for w in words if len(w) >= 2}
        return keywords
    
    def _chunk_text(self, text: str, chunk_size: int = 400) -> list:
        """å°†æ–‡æœ¬åˆ†å‰²æˆ chunks"""
        lines = text.split('\n')
        chunks = []
        current_chunk = []
        current_size = 0
        
        for line in lines:
            current_chunk.append(line)
            current_size += len(line)
            if current_size >= chunk_size:
                chunks.append('\n'.join(current_chunk))
                # ä¿ç•™æœ€åä¸€è¡Œä½œä¸º overlap
                current_chunk = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                current_size = 0
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
    
    def _keyword_filter(self, query: str, files: list) -> list:
        """é˜¶æ®µ1: å…³é”®è¯å¿«é€Ÿè¿‡æ»¤"""
        query_keywords = self._extract_keywords(query)
        if not query_keywords:
            return files
        
        candidates = []
        for file_path in files:
            if not file_path.exists():
                continue
            content = file_path.read_text(encoding="utf-8").lower()
            file_keywords = self._extract_keywords(content)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
            if query_keywords & file_keywords:
                candidates.append(file_path)
        
        return candidates
    
    def _vector_rerank(self, query: str, files: list, threshold: float = 0.3, top_k: int = 3) -> list:
        """é˜¶æ®µ2: å‘é‡é‡æ’ + é˜ˆå€¼è¿‡æ»¤"""
        if not files:
            return []
        
        query_embedding = embeddings.embed_query(query)
        
        scored_files = []
        for file_path in files:
            content = file_path.read_text(encoding="utf-8")
            if not content.strip():
                continue
            
            # åˆ† chunk
            chunks = self._chunk_text(content)
            chunk_scores = []
            
            for chunk in chunks:
                chunk_embedding = embeddings.embed_query(chunk)
                similarity = np.dot(query_embedding, chunk_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding) + 1e-8
                )
                chunk_scores.append((similarity, chunk))
            
            if chunk_scores:
                # å–æœ€é«˜ç›¸ä¼¼åº¦
                best_score = max(chunk_scores, key=lambda x: x[0])
                scored_files.append((best_score[0], file_path.name, best_score[1]))
        
        # æ’åºå¹¶è¿‡æ»¤
        scored_files.sort(key=lambda x: x[0], reverse=True)
        results = [(score, name, chunk) for score, name, chunk in scored_files if score >= threshold]
        
        return results[:top_k]
    
    def add_message(self, role: str, content: str):
        """æ·»åŠ å¯¹è¯æ¶ˆæ¯åˆ°å½“æ—¥æ—¥å¿—"""
        today_file = self._get_today_file()
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # åˆå§‹åŒ–æ–‡ä»¶
        if not today_file.exists():
            today_file.write_text(f"# {datetime.now().strftime('%Y-%m-%d')} å¯¹è¯æ—¥å¿—\n\n", encoding="utf-8")
        
        content_md = today_file.read_text(encoding="utf-8")
        content_md += f"- **{timestamp} {role}**: {content}\n\n"
        
        today_file.write_text(content_md, encoding="utf-8")
        
        # è®¡æ•°
        self.conversation_count += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ compaction
        if self.conversation_count >= self.compaction_interval:
            self.compact()
            self.conversation_count = 0
    
    def retrieve_relevant_history(self, query: str, top_k: int = 3, threshold: float = 0.3) -> str:
        """æ··åˆæ£€ç´¢: å…³é”®è¯è¿‡æ»¤ + å‘é‡é‡æ’"""
        # æ”¶é›†è¦æœç´¢çš„æ–‡ä»¶
        search_files = [self.memory_file, self.soul_file, self.agents_file]
        
        # æ·»åŠ æœ€è¿‘ N å¤©çš„æ—¥å¿—ï¼ˆæœ€å¤š7å¤©ï¼‰
        days_to_search = 7
        for i in range(days_to_search):
            day = datetime.now() - timedelta(days=i)
            day_file = self.daily_dir / f"{day.strftime('%Y-%m-%d')}.md"
            search_files.append(day_file)
        
        # é˜¶æ®µ1: å…³é”®è¯è¿‡æ»¤
        candidates = self._keyword_filter(query, search_files)
        
        # é˜¶æ®µ2: å‘é‡é‡æ’
        results = self._vector_rerank(query, candidates, threshold, top_k)
        
        if not results:
            return ""
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = []
        for score, name, chunk in results:
            formatted.append(f"<memory-snippet file=\"{name}\" score=\"{score:.3f}\">\n{chunk}\n</memory-snippet>")
        
        return "\n\n".join(formatted)
    
    def compact(self):
        """å®šæœŸå°†é‡è¦ä¿¡æ¯å‹ç¼©åˆ°é•¿æœŸè®°å¿†"""
        # è¯»å–æœ€è¿‘å‡ å¤©çš„æ—¥å¿—
        recent_content = []
        for i in range(3):  # æœ€è¿‘3å¤©
            day = datetime.now() - timedelta(days=i)
            day_file = self.daily_dir / f"{day.strftime('%Y-%m-%d')}.md"
            if day_file.exists():
                content = day_file.read_text(encoding="utf-8")
                if content.strip():
                    recent_content.append(content)
        
        if not recent_content:
            return
        
        # è¯»å–ç°æœ‰è®°å¿†
        memory_content = self.memory_file.read_text(encoding="utf-8")
        
        # ç®€å•è¿½åŠ ç­–ç•¥ï¼šä¿ç•™æœ€è¿‘å¯¹è¯çš„æ‘˜è¦
        memory_content += f"\n### {datetime.now().strftime('%Y-%m-%d')} æ‘˜è¦\n"
        memory_content += "ï¼ˆè¿‘æœŸå¯¹è¯å·²æ•´åˆï¼‰\n"
        
        self.memory_file.write_text(memory_content, encoding="utf-8")
        print("ğŸ”„ Memory compaction å®Œæˆ")
    
    def get_soul(self) -> str:
        """è·å–çµé­‚é…ç½®"""
        return self.soul_file.read_text(encoding="utf-8") if self.soul_file.exists() else ""
    
    def get_agents(self) -> str:
        """è·å– Agent è§„èŒƒ"""
        return self.agents_file.read_text(encoding="utf-8") if self.agents_file.exists() else ""
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        # æ¸…ç©ºæ¯æ—¥æ—¥å¿—
        for f in self.daily_dir.glob("*.md"):
            f.unlink()
        
        # é‡ç½®é•¿æœŸè®°å¿†ï¼ˆä¿ç•™ç»“æ„ï¼‰
        self.memory_file.write_text("# é•¿æœŸè®°å¿†\n\n## ç”¨æˆ·åå¥½\n\n## æ ¸å¿ƒäº‹å®\n\n## å…³é”®å†³ç­–\n\n", encoding="utf-8")
        self.conversation_count = 0
        print("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…ç©º")

# =========================
# ğŸ”¹ åˆå§‹åŒ–ç»„ä»¶
# =========================
memory_manager = MemoryManager()

vector_db = ElasticsearchStore(
    es_url=os.getenv('ES_LOCAL_URL'),
    embedding=embeddings,
    index_name=index_name
)

# å®šä¹‰ Promptï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
prompt_template = PromptTemplate.from_template(
    template="""Previous conversation:
{history}

[DOCUMENT FRAGMENTS START]
{context}
[DOCUMENT FRAGMENTS END]

[USER QUESTION START]
{question}
[USER QUESTION END]

Instructions:
1. The text above in [DOCUMENT FRAGMENTS START]...[DOCUMENT FRAGMENTS END] contains retrieved document fragments for reference only.
2. The text above in [USER QUESTION START]...[USER QUESTION END] is the user's question.
3. Answer the user's question based on the document fragments when relevant, otherwise use your own knowledge.
4. CRITICAL: Answer in the SAME LANGUAGE as the user's question, NOT the language of the document fragments.
5. Write only three sentences."""
)

# å®šä¹‰çŠ¶æ€
class State(TypedDict):
    question: str
    top_k: int
    context: List[Document]
    history: str
    answer: str

# å®šä¹‰åº”ç”¨æ­¥éª¤
def retrieve(state: State):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£å’Œå¯¹è¯å†å²"""
    # æ£€ç´¢æ–‡æ¡£ï¼ˆå¸¦ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰ï¼Œä½¿ç”¨ä¼ å…¥çš„ top_k
    top_k = state.get("top_k", 3)
    retrieved_docs_with_scores = vector_db.similarity_search_with_score(state["question"], k=top_k)
    
    # æ£€ç´¢ç›¸å…³å¯¹è¯å†å²
    relevant_history = memory_manager.retrieve_relevant_history(state["question"], top_k=3)
    
    return {"context": retrieved_docs_with_scores, "history": relevant_history}


def generate(state: State):
    """ç”Ÿæˆç­”æ¡ˆ"""
    # é˜ˆå€¼è®¾ç½®ï¼šæ–‡æ¡£ç›¸ä¼¼åº¦é˜ˆå€¼å’Œæ•´ä½“æ„å›¾åˆ¤æ–­é˜ˆå€¼
    DOC_SIMILARITY_THRESHOLD = 0.75
    INTENT_SIMILARITY_THRESHOLD = 0.7

    # å¤„ç†å¸¦åˆ†æ•°çš„æ–‡æ¡£ï¼ˆ(doc, score) å…ƒç»„åˆ—è¡¨ï¼‰ï¼Œè¿‡æ»¤ä½ç›¸ä¼¼åº¦
    context_docs = []
    all_scores = []

    # å…ˆå½’ä¸€åŒ–åˆ†æ•°
    context_items = state.get("context", [])
    if context_items:
        # æå–åˆ†æ•°å¹¶å½’ä¸€åŒ–
        scored_docs = []
        for item in context_items:
            if isinstance(item, tuple):
                doc, score = item
                scored_docs.append((doc, score))

        if scored_docs:
            raw_scores = [s for _, s in scored_docs]
            all_scores = raw_scores
            max_s, min_s = max(raw_scores), min(raw_scores)

            # åˆ¤æ–­æ˜¯è·ç¦»è¿˜æ˜¯ç›¸ä¼¼åº¦ï¼šè·ç¦»é€šå¸¸ > 1ï¼Œç›¸ä¼¼åº¦é€šå¸¸ <= 1
            is_distance = max_s > 1.0

            for doc, score in scored_docs:
                if is_distance:
                    # è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦: similarity = 1 / (1 + distance)
                    normalized = 1.0 / (1.0 + score)
                else:
                    # å·²ç»æ˜¯ç›¸ä¼¼åº¦ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–
                    normalized = score

                # è®°å½•å½’ä¸€åŒ–åçš„ç›¸ä¼¼åº¦
                doc.metadata["similarity"] = normalized

                if normalized >= DOC_SIMILARITY_THRESHOLD:
                    context_docs.append(doc)

    # æ„å›¾åˆ¤æ–­ï¼šè®¡ç®—æœ€é«˜ç›¸ä¼¼åº¦
    max_similarity = 0
    if all_scores:
        max_raw = max(all_scores)
        min_raw = min(all_scores)
        is_distance = max_raw > 1.0
        if is_distance:
            max_similarity = 1.0 / (1.0 + min_raw)  # æœ€å°è·ç¦»å¯¹åº”æœ€é«˜ç›¸ä¼¼åº¦
        else:
            max_similarity = max_raw  # ç›´æ¥ä½¿ç”¨åŸå§‹ç›¸ä¼¼åº¦

    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ£€ç´¢ç»“æœ
    use_retrieved_context = max_similarity >= INTENT_SIMILARITY_THRESHOLD

    if use_retrieved_context and context_docs:
        docs_content = "\n\n".join(doc.page_content for doc in context_docs)
        context_info = f"ï¼ˆä½¿ç”¨äº† {len(context_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}ï¼‰"
    else:
        docs_content = ""
        if max_similarity < INTENT_SIMILARITY_THRESHOLD:
            context_info = f"ï¼ˆæ£€ç´¢åˆ°çš„æ–‡æ¡£ç›¸å…³æ€§ä¸è¶³ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}ï¼‰ï¼Œä¸ä½¿ç”¨æ£€ç´¢ç»“æœï¼‰"
        else:
            context_info = "ï¼ˆæœªæ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µï¼‰"

    history = state.get("history", "") or "No previous conversation."

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡è°ƒæ•´æç¤ºè¯
    if use_retrieved_context and docs_content:
        prompt = prompt_template.format(
            question=state["question"],
            context=docs_content,
            history=history
        )
    else:
        # ä¸ä½¿ç”¨æ£€ç´¢ç»“æœï¼Œç›´æ¥åŸºäºæ¨¡å‹çŸ¥è¯†å›ç­”
        no_context_prompt = PromptTemplate.from_template(
            template="""Previous conversation:
{history}

[USER QUESTION START]
{question}
[USER QUESTION END]

Instructions:
1. The retrieved documents are not relevant to this question.
2. Answer based on your own knowledge.
3. CRITICAL: Answer in the SAME LANGUAGE as the user's question.
4. Write only three sentences."""
        )
        prompt = no_context_prompt.format(
            question=state["question"],
            history=history
        )

    response = llm.invoke(prompt)

    # è®°å½•åˆ°æ£€ç´¢æ—¥å¿—
    retrieval_logger.log(
        question=state["question"],
        retrieved_docs=context_items,
        answer=response.content,
        used_context=use_retrieved_context
    )

    # åœ¨ç­”æ¡ˆä¸­æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯è¯´æ˜ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼Œå¯ç§»é™¤ï¼‰
    final_answer = response.content
    # final_answer = f"{response.content}\n\n{context_info}"  # å–æ¶ˆæ³¨é‡Šå¯æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯

    return {"answer": final_answer}


# ç¼–è¯‘åº”ç”¨
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

# =========================
# ğŸ”¹ å¯¹è¯å‡½æ•°ï¼ˆä¾› Streamlit è°ƒç”¨ï¼‰
# =========================
def ask_question(question: str, top_k: int = 3):
    """
    é—®ç­”å‡½æ•°ï¼Œä¾› Web ç•Œé¢è°ƒç”¨

    Args:
        question: ç”¨æˆ·é—®é¢˜
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡

    Returns:
        dict: åŒ…å« answer, source, question, used_context
    """
    # è°ƒç”¨å›¾ï¼Œä¼ é€’ top_k å‚æ•°
    response = graph.invoke({"question": question, "top_k": top_k})

    # ä¿å­˜å¯¹è¯å†å²åˆ° Markdown
    memory_manager.add_message("ç”¨æˆ·", question)
    memory_manager.add_message("AI", response["answer"])

    # æ•´ç†ç»“æœï¼ˆå¤„ç†å¸¦åˆ†æ•°çš„æ–‡æ¡£ï¼‰
    sources = []
    context_items = response.get("context", [])

    # æå–æ‰€æœ‰åˆ†æ•°
    all_scores = []
    for item in context_items:
        if isinstance(item, tuple):
            _, score = item
            all_scores.append(score)

    # åˆ¤æ–­æ˜¯è·ç¦»è¿˜æ˜¯ç›¸ä¼¼åº¦
    has_scores = bool(all_scores)
    is_distance = False
    if has_scores:
        max_score_val = max(all_scores)
        is_distance = max_score_val > 1.0

    # ä½¿ç”¨ä¸ generate å‡½æ•°ç›¸åŒçš„é˜ˆå€¼
    DOC_SIMILARITY_THRESHOLD = 0.75

    # è¿‡æ»¤å¹¶å¤„ç†æ–‡æ¡£ç‰‡æ®µ
    for item in context_items:
        if isinstance(item, tuple):
            doc, score = item
            # åˆ¤æ–­æ˜¯è·ç¦»è¿˜æ˜¯ç›¸ä¼¼åº¦
            if has_scores:
                if is_distance:
                    # è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦: similarity = 1 / (1 + distance)
                    normalized_score = 1.0 / (1.0 + score)
                else:
                    # å·²ç»æ˜¯ç›¸ä¼¼åº¦ï¼Œç›´æ¥ä½¿ç”¨
                    normalized_score = score
            else:
                normalized_score = 0.5

            # åªæ·»åŠ è¾¾åˆ°æ–‡æ¡£ç›¸ä¼¼åº¦é˜ˆå€¼çš„æ–‡æ¡£
            if normalized_score >= DOC_SIMILARITY_THRESHOLD:
                sources.append({
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", "unknown"),
                    "page": doc.metadata.get("page_label", "unknown"),
                    "similarity": normalized_score
                })

    # æ ¹æ® sources æ˜¯å¦ä¸ºç©ºåˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº†æ£€ç´¢ç»“æœ
    used_context = len(sources) > 0

    return {
        "question": question,
        "answer": response["answer"],
        "source": sources,
        "used_context": used_context
    }


def clear_conversation_history():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    memory_manager.clear_history()


def create_rag_chain():
    """åˆ›å»ºå¹¶è¿”å› RAG é“¾ï¼Œä¾›è¯„ä¼°å™¨ä½¿ç”¨

    Returns:
        compiled graph: ç¼–è¯‘å¥½çš„ LangGraph
    """
    return graph

# =========================
# ğŸ”¹ ä¸»å‡½æ•°ï¼ˆå‘½ä»¤è¡Œæµ‹è¯•ï¼‰
# =========================
if __name__ == "__main__":
    # æµ‹è¯•ç”¨ï¼Œè¯·ä¿®æ”¹é—®é¢˜åè¿è¡Œ
    pass
