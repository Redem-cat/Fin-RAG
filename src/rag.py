# RAG architecture using LangChain, Ollama and Elasticsearch
# Modified by Redem-cat

import os
import pickle
import re
import string
from pathlib import Path

import jieba
from dotenv import load_dotenv

from langchain_elasticsearch import ElasticsearchStore
from langchain_ollama import OllamaEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict

# =========================
# ğŸ”¹ é…ç½®å’Œåˆå§‹åŒ–
# =========================
base_path = Path(__file__).parent.parent.resolve()

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv_path = Path(base_path / "elastic-start-local/.env")
if not dotenv_path.is_file():
    print("Error: it seems Elasticsearch has not been installed")
    print("using start-local, please execute the following command:")
    print("curl -fsSL https://elastic.co/start-local | sh")
    exit(1)
    
load_dotenv(dotenv_path=dotenv_path)
index_name = "rag-langchain"

# Embeddings
embeddings = OllamaEmbeddings(
    model="my-bge-m3",
)

# LLM
llm = ChatOllama(model="llama3.2:3b", temperature=0.0000000001)

# =========================
# ğŸ”¹ åœç”¨è¯å¤„ç†ç±»
# =========================
class ChineseTextProcessor:
    """ä¸­æ–‡æ–‡æœ¬å¤„ç†å™¨ï¼šåˆ†è¯ + åœç”¨è¯è¿‡æ»¤"""
    
    def __init__(self, stopwords_file: str = None):
        self.stopwords = self.load_stopwords(stopwords_file)
    
    def load_stopwords(self, stopwords_file: str = None):
        """åŠ è½½åœç”¨è¯åº“"""
        stopwords = set()
        if stopwords_file is None:
            stopwords_file = base_path / "ä¸­æ–‡åœç”¨è¯åº“.txt"
        
        stopwords_path = Path(stopwords_file)
        if stopwords_path.exists():
            with open(stopwords_path, "r", encoding="utf-8") as f:
                stopwords = {line.strip() for line in f if line.strip()}
            print(f"[OK] Loaded {len(stopwords)} stopwords")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°åœç”¨è¯åº“æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤åœç”¨è¯")
            stopwords.update({
                "çš„", "äº†", "å’Œ", "æ˜¯", "åœ¨", "æˆ‘", "æœ‰", "å°±", "ä¸", "äºº",
                "éƒ½", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»",
                "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "è‡ªå·±", "è¿™", "é‚£", "è¿˜", "ä»€ä¹ˆ"
            })
        return stopwords
    
    def process(self, text: str) -> str:
        """å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯"""
        if not text:
            return text
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡æ–‡æœ¬
        if re.search(r'[\u4e00-\u9fff]', text):
            words = jieba.cut(text)
            cleaned_words = []
            for word in words:
                word = word.strip()
                if not word or word in self.stopwords:
                    continue
                if word in string.punctuation or re.match(r"^[\W_]+$", word):
                    continue
                if len(word) == 1:
                    continue
                cleaned_words.append(word)
            return " ".join(cleaned_words)
        
        # è‹±æ–‡æ–‡æœ¬ç›´æ¥è¿”å›
        return text

# =========================
# ğŸ”¹ ç¼“å­˜ç®¡ç†ç±»
# =========================
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼šç®¡ç†æ–‡æ¡£åˆ†å—å’Œå‘é‡ç´¢å¼•ç¼“å­˜"""
    
    def __init__(self, cache_dir: str = None):
        if cache_dir is None:
            cache_dir = base_path / "cache"
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.chunks_cache = self.cache_dir / "doc_chunks.pkl"
        self.vectorizer_cache = self.cache_dir / "vectorizer_cache.pkl"
        self.vector_matrix_cache = self.cache_dir / "vector_matrix_cache.pkl"
    
    def save_chunks(self, chunks: List[Document]):
        """ä¿å­˜æ–‡æ¡£åˆ†å—åˆ°ç¼“å­˜"""
        with open(self.chunks_cache, "wb") as f:
            pickle.dump(chunks, f)
        print(f"ğŸ’¾ å·²ç¼“å­˜ {len(chunks)} ä¸ªæ–‡æ¡£åˆ†å—")
    
    def load_chunks(self) -> List[Document]:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£åˆ†å—"""
        if self.chunks_cache.exists():
            with open(self.chunks_cache, "rb") as f:
                chunks = pickle.load(f)
            print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(chunks)} ä¸ªæ–‡æ¡£åˆ†å—")
            return chunks
        return None
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        for cache_file in [self.chunks_cache, self.vectorizer_cache, self.vector_matrix_cache]:
            if cache_file.exists():
                cache_file.unlink()
        print("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…é™¤")

# =========================
# ğŸ”¹ åˆå§‹åŒ–ç»„ä»¶
# =========================
text_processor = ChineseTextProcessor()
cache_manager = CacheManager()

vector_db = ElasticsearchStore(
    es_url=os.getenv('ES_LOCAL_URL'),
    embedding=embeddings,
    index_name=index_name
)

# å®šä¹‰ Prompt
prompt_template = PromptTemplate.from_template(
    template="Given the following context: {context}, answer to the following question: {question}. Write only three sentences."
)

# å®šä¹‰çŠ¶æ€
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

# å®šä¹‰åº”ç”¨æ­¥éª¤
def retrieve(state: State):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    # å¯¹æŸ¥è¯¢è¿›è¡Œåœç”¨è¯å¤„ç†
    processed_query = text_processor.process(state["question"])
    
    # å¦‚æœå¤„ç†åçš„æŸ¥è¯¢ä¸åŸæŸ¥è¯¢ä¸åŒï¼Œæ‰“å°æ—¥å¿—
    if processed_query != state["question"]:
        print(f"ğŸ” æŸ¥è¯¢å¤„ç†: '{state['question']}' -> '{processed_query}'")
    
    retrieved_docs = vector_db.similarity_search(processed_query, k=8)
    return {"context": retrieved_docs}


def generate(state: State):
    """ç”Ÿæˆç­”æ¡ˆ"""
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    prompt = prompt_template.format(question=state["question"], context=docs_content) 
    response = llm.invoke(prompt)
    return {"answer": response.content}


# ç¼–è¯‘åº”ç”¨
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

# =========================
# ğŸ”¹ å¯¹è¯å‡½æ•°ï¼ˆä¾› Streamlit è°ƒç”¨ï¼‰
# =========================
def ask_question(question: str, top_k: int = 8):
    """
    é—®ç­”å‡½æ•°ï¼Œä¾› Web ç•Œé¢è°ƒç”¨
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
    
    Returns:
        dict: åŒ…å« answer, source, question
    """
    # ä¸´æ—¶ä¿®æ”¹ k å€¼
    original_k = 8
    
    # è°ƒç”¨å›¾
    response = graph.invoke({"question": question})
    
    # æ•´ç†ç»“æœ
    sources = []
    for doc in response.get("context", []):
        sources.append({
            "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
            "source": doc.metadata.get("source", "unknown"),
            "similarity": 0.9  # ES ä¸è¿”å›ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼
        })
    
    return {
        "question": question,
        "answer": response["answer"],
        "source": sources
    }

# =========================
# ğŸ”¹ ä¸»å‡½æ•°ï¼ˆå‘½ä»¤è¡Œæµ‹è¯•ï¼‰
# =========================
if __name__ == "__main__":
    question = "Who won the Nobel Prize in Physics 2024?"
    print(question)
    response = graph.invoke({"question": question})
    print(response["answer"])
