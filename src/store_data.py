# Store PDF documents in Elasticsearch using Ollama embeddings
# Modified by Redem-cat

import glob, os
from dotenv import load_dotenv
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain_elasticsearch import ElasticsearchStore
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter


# Load and chunk contents of the PDF
base_path = Path(__file__).parent.parent.resolve()
  
# Index the chunks in Elasticsearch
dotenv_path = Path(base_path / "elastic-start-local/.env")
if not dotenv_path.is_file():
    print("Error: it seems Elasticsearch has not been installed")
    print("using start-local, please execute the following command:")
    print("curl -fsSL https://elastic.co/start-local | sh")
    exit(1)
    
load_dotenv(dotenv_path=dotenv_path)
index_name="rag-langchain"

# Embeddings
embeddings = OllamaEmbeddings(
    model="my-bge-m3",
)

vector_db  = ElasticsearchStore(
    es_url=os.getenv('ES_LOCAL_URL'),

    embedding=embeddings,
    index_name=index_name
)

# =========================
# ğŸ”¹ ç¼“å­˜ç®¡ç†
# =========================
cache_dir = base_path / "cache"
cache_dir.mkdir(exist_ok=True)
chunks_cache = cache_dir / "doc_chunks.pkl"

# å°è¯•ä»ç¼“å­˜åŠ è½½å·²å¤„ç†çš„æ–‡æ¡£åˆ†å—
cached_chunks = None
if chunks_cache.exists():
    print("ğŸ’¾ å‘ç°æ–‡æ¡£åˆ†å—ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½...")
    try:
        with open(chunks_cache, "rb") as f:
            cached_chunks = pickle.load(f)
        print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(cached_chunks)} ä¸ªåˆ†å—")
    except Exception as e:
        print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        cached_chunks = None

# Check if the index already exists
res = vector_db.client.indices.exists(index=index_name)
if res.body:
    if cached_chunks:
        print(f"ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œä¸”æœ‰ç¼“å­˜ï¼Œè·³è¿‡å¤„ç†")
    else:
        print(f"ç´¢å¼• {index_name} å·²å­˜åœ¨äº Elasticsearch")
        exit(1)
    
# =========================
# ğŸ”¹ å¤„ç†æ–‡æ¡£
# =========================
if cached_chunks is None:
    print(f"Reading the PDFs in {base_path}/data")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    # Read the PDF files and split into chunks
    all_splits = []
    for file in glob.glob(f"{base_path}/data/*.pdf"):
        loader = PyPDFLoader(file)
        docs = loader.load()
        pages=len(docs)
        print(f"Read {file} with {pages} pages")
        chunks = text_splitter.split_documents(docs)
        num_chunks=len(chunks)
        print(f"Splitted in {num_chunks} chunks")
        all_splits.append(chunks)
    
    # åˆå¹¶æ‰€æœ‰åˆ†å—
    all_chunks = []
    for chunks in all_splits:
        all_chunks.extend(chunks)
    
    # ä¿å­˜åˆ°ç¼“å­˜
    try:
        with open(chunks_cache, "wb") as f:
            pickle.dump(all_chunks, f)
        print(f"ğŸ’¾ å·²ç¼“å­˜ {len(all_chunks)} ä¸ªæ–‡æ¡£åˆ†å—")
    except Exception as e:
        print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
else:
    all_chunks = cached_chunks
    print(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜ä¸­çš„ {len(all_chunks)} ä¸ªæ–‡æ¡£åˆ†å—")
            
print(f"Storing chunks in Elasticsearch")
# Index the chunks to Elasticsearch
vector_db.add_documents(all_chunks)
print(f"Stored {len(all_chunks)} chunks in {index_name} index")

