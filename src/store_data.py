# Store PDF documents in Elasticsearch using Ollama embeddings
# Modified by Redem-cat

import glob, os
from dotenv import load_dotenv
from pathlib import Path
from langchain_core.documents import Document
from langchain_elasticsearch import ElasticsearchStore
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from docling.document_converter import DocumentConverter
from docling.datamodel.base_models import InputFormat


# Load and chunk contents of the PDF
base_path = Path(__file__).parent.parent.resolve()

# Index the chunks in Elasticsearch
dotenv_path = Path(base_path / "elastic-start-local/.env")
if not dotenv_path.is_file():
    print("Error: it seems Elasticsearch has not been installed")
    print("using start-local, please execute the following command:")
    print("curl -fsSL https://elastic.co/start-local | sh")
    exit(1)
    
load_dotenv(dotenv_path=dotenv_path)
index_name = "rag-langchain"

# Embeddings
embeddings = OllamaEmbeddings(
    model="my-bge-m3",
)

vector_db = ElasticsearchStore(
    es_url=os.getenv('ES_LOCAL_URL'),
    embedding=embeddings,
    index_name=index_name
)

# =========================
# ğŸ”¹ æ£€æŸ¥ç´¢å¼•çŠ¶æ€
# =========================
res = vector_db.client.indices.exists(index=index_name)
if res.body:
    print(f"ç´¢å¼• {index_name} å·²å­˜åœ¨äº Elasticsearch")
    print("å¦‚éœ€é‡æ–°å¯¼å…¥ï¼Œè¯·å…ˆåˆ é™¤ç´¢å¼•æˆ–é‡å»º")
    exit(0)

# =========================
# ğŸ”¹ å¤„ç†æ–‡æ¡£
# =========================
print(f"Reading the PDFs in {base_path}/data")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800, 
    chunk_overlap=100,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼š", "ï¼Œ", "ã€", " ", ""]
)

# Read the PDF files and split into chunks
converter = DocumentConverter()
all_splits = []

for file in glob.glob(f"{base_path}/data/*.pdf"):
    # ä½¿ç”¨ Docling åŠ è½½ PDF
    print(f"Reading {file}")
    docling_doc = converter.convert(file)

    # è½¬æ¢ä¸º LangChain Document æ ¼å¼
    pages = len(docling_doc.pages) if hasattr(docling_doc, 'pages') else 1
    print(f"Read {file} with {pages} pages")

    # Docling æä¾›çš„ markdown å†…å®¹
    markdown_text = docling_doc.export_to_markdown()

    # åˆ›å»º Document å¯¹è±¡
    doc = Document(
        page_content=markdown_text,
        metadata={"source": file, "file_type": "pdf"}
    )

    # åˆ†å—
    chunks = text_splitter.split_documents([doc])
    num_chunks = len(chunks)
    print(f"Splitted in {num_chunks} chunks")
    all_splits.append(chunks)

# åˆå¹¶æ‰€æœ‰åˆ†å—
all_chunks = []
for chunks in all_splits:
    all_chunks.extend(chunks)

print(f"Storing chunks in Elasticsearch")
# Index the chunks to Elasticsearch
vector_db.add_documents(all_chunks)
print(f"Stored {len(all_chunks)} chunks in {index_name} index")
